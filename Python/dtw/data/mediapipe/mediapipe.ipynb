{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using mediapipe for motion tracking\n",
    "\n",
    "In this module, we learn how to run mediapipe to automatically track movements in videos. <br>\n",
    "This notebook is based on https://github.com/WimPouw/envisionBOX_modulesWP/blob/main/MultimodalMerging/Masking_Mediapiping.ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Overview of the script\n",
    "The script performs following processes in the listed order:\n",
    "\n",
    "1. Import packages and define path for input and output folders\n",
    "1. Check how many video files are in the input folder\n",
    "1. Load mediapipe module and define landmarks (which part of the body do we want track?)\n",
    "1. Make empty objects (similar to dataframes) with column names for face, body, and hands separetely <-- later we will add data to them in step 6-1\n",
    "1. Make a folder for each video\n",
    "1. Perform mediapipe frame-by-frame for each video file\n",
    "    1. For each frame, add coordinates of each landmarks as a row to the face, body, and hands objects\n",
    "    1. Once processed all the frames in the video, save the face, body, and hands objects as csv files in the output folder\n",
    "    1. Move onto the next video file (if any)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages and define path for input and output folders\n",
    "Let's first import required packages.\n",
    "\n",
    "<font color = \"mandarin\">If you haven't installed the packages, follow the steps below</font>\n",
    "\n",
    "1. Open terminal/anaconda prompt at the folder in which you store this notebook\n",
    "    - Mac:\n",
    "        1. go to the \"mediapipe\" folder\n",
    "        1. right-click the \"mediapipe\" folder\n",
    "        1. click \"open terminal at this folder\" <br><br>\n",
    "    - Windows:\n",
    "        1. go to the \"mediapipe\" folder\n",
    "        1. copy the path to the folder\n",
    "        1. open Anaconda Prompt\n",
    "        1. type cd and paste the path after a space (e.g., cd D:/users/shoakamine/mld_study_group/Python/mediapipe)\n",
    "        1. type D:\n",
    "1. Activate your conda environment (e.g., conda activate mld_study_group)\n",
    "1. Run this code: pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['001', '002', '003', '005', '006', '007', '008', '009', '010', '011', '012', '013', '015', '016', '017', '018', '019', '020', '021', '022', '023', '024', '025', '026', '027', '028', '029', '030', '031', '032', '033', '034', '035', '036', '037', '038', '039', '040', '041', '042', '043', '044', '045', '046', '047']\n"
     ]
    }
   ],
   "source": [
    "#load in required packages\n",
    "import mediapipe as mp #mediapipe\n",
    "import cv2 #opencv\n",
    "import math #basic operations\n",
    "import numpy as np #basic operations\n",
    "import pandas as pd #data wrangling\n",
    "import csv #csv saving\n",
    "import os #some basic functions for inspecting folder structure etc.\n",
    "import time as tm #for timing the processing time\n",
    "from pathlib import Path #for setting paths\n",
    "from os import listdir\n",
    "from os.path import isfile, join, exists\n",
    "from tqdm import tqdm #for progress bars\n",
    "\n",
    "\n",
    "### define folders\n",
    "cwd = Path(os.getcwd())\n",
    "root_f = cwd.parent.parent.parent.parent.absolute() #get the path to three folders above the current folder (root folder)\n",
    "media_f = os.path.join(root_f, \"05_data\", \"01_interaction\", \"02_media\")\n",
    "# list up all the folders that doesn't have \"_\" and \"p\" in the folder name\n",
    "media_folders = [f for f in os.listdir(media_f) if \"_\" not in f and \"p\" not in f]\n",
    "media_folders = media_folders[:-1] #remove the last folder\n",
    "print(media_folders)\n",
    "\n",
    "input_flipped = \"flipped_videos/\"\n",
    "outputf_video = \"output_videos/\"\n",
    "outputf_ts = \"output_timeseries/\"\n",
    "outputf_video_flipped = \"output_videos_flipped/\"\n",
    "outputf_ts_flipped = \"output_timeseries_flipped/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check number of videos files to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following folder is set as the output folder where all the pose time series are stored\n",
      "p:\\workspaces\\mld-akamine\\working_data\\01_fribble_zoom_experiment\\08_analysis\\kinematics\\data\\mediapipe\\output_timeseries\n",
      "\n",
      " The following folder is set as the output folder for saving the masked videos \n",
      "p:\\workspaces\\mld-akamine\\working_data\\01_fribble_zoom_experiment\\08_analysis\\kinematics\\data\\mediapipe\\output_videos\n",
      "\n",
      " The following 46 videos will be processed: \n",
      "['001_a.mp4', '001_b.mp4', '002_a.mp4', '002_b.mp4', '003_a.mp4', '003_b.mp4', '005_a.mp4', '005_b.mp4', '006_a.mp4', '006_b.mp4', '007_a.mp4', '007_b.mp4', '008_a.mp4', '008_b.mp4', '009_a.mp4', '009_b.mp4', '010_a.mp4', '010_b.mp4', '011_a.mp4', '011_b.mp4', '012_a.mp4', '012_b.mp4', '013_a.mp4', '013_b.mp4', '015_a.mp4', '015_b.mp4', '016_a.mp4', '016_b.mp4', '017_a.mp4', '017_b.mp4', '018_a.mp4', '018_b.mp4', '019_a.mp4', '019_b.mp4', '020_a.mp4', '020_b.mp4', '021_a.mp4', '021_b.mp4', '022_a.mp4', '022_b.mp4', '023_a.mp4', '023_b.mp4', '024_a.mp4', '024_b.mp4', '025_a.mp4', '025_b.mp4']\n",
      "\n",
      " The following 0 flipped videos will be processed: \n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# vfiles = [f for f in listdir(inputf_video) if join(inputf_video, f).endswith(\".mp4\")] #loop through the filenames and collect them in a list\n",
    "vfiles = []\n",
    "vfilenames = []\n",
    "vfiles_flipped = []\n",
    "vfilenames_flipped = []\n",
    "\n",
    "# iterate over files in the videos folder and make a list of the video files to be processed\n",
    "for foldername in media_folders:\n",
    "    processed_folder = os.path.join(media_f, foldername, \"processed\")\n",
    "\n",
    "    if any(fn.startswith(foldername) for fn in os.listdir(outputf_ts)):\n",
    "        continue # skip if the file already exists\n",
    "\n",
    "    for filename in os.listdir(processed_folder):\n",
    "        if filename.endswith(\".mp4\"):\n",
    "            vfiles.append(os.path.join(processed_folder, filename))\n",
    "            vfilenames.append(filename)\n",
    "\n",
    "### for flipped videos\n",
    "for filename in os.listdir(input_flipped):\n",
    "    if filename.endswith(\".mp4\") and not filename.split(\".mp4\")[0] in os.listdir(outputf_ts_flipped):\n",
    "        vfiles_flipped.append(os.path.join(input_flipped, filename))\n",
    "        vfilenames_flipped.append(filename)\n",
    "\n",
    "#check videos to be processed\n",
    "print(\"The following folder is set as the output folder where all the pose time series are stored\")\n",
    "print(os.path.abspath(outputf_ts))\n",
    "print(\"\\n The following folder is set as the output folder for saving the masked videos \")\n",
    "print(os.path.abspath(outputf_video))\n",
    "print(f\"\\n The following {len(vfilenames)} videos will be processed: \")\n",
    "print(vfilenames)\n",
    "print(f\"\\n The following {len(vfilenames_flipped)} flipped videos will be processed: \")\n",
    "print(vfilenames_flipped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize mediapipe modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note that we have the following number of pose keypoints for markers body\n",
      "33\n",
      "\n",
      " Note that we have the following number of pose keypoints for markers hands\n",
      "42\n",
      "\n",
      " Note that we have the following number of pose keypoints for markers face\n",
      "478\n"
     ]
    }
   ],
   "source": [
    "### initialize modules and functions=======================\n",
    "\n",
    "#load in mediapipe modules\n",
    "mp_holistic = mp.solutions.holistic\n",
    "# Import drawing_utils and drawing_styles.\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "\n",
    "### Define landmarks=====================================\n",
    "#landmarks 33x that are used by Mediapipe (Blazepose)\n",
    "markersbody = ['NOSE', 'LEFT_EYE_INNER', 'LEFT_EYE', 'LEFT_EYE_OUTER', 'RIGHT_EYE_INNER', \n",
    "                'RIGHT_EYE', 'RIGHT_EYE_OUTER', 'LEFT_EAR', 'RIGHT_EAR', 'MOUTH_LEFT', \n",
    "                'MOUTH_RIGHT', 'LEFT_SHOULDER', 'RIGHT_SHOULDER', 'LEFT_ELBOW', 'RIGHT_ELBOW', \n",
    "                'LEFT_WRIST', 'RIGHT_WRIST', 'LEFT_PINKY', 'RIGHT_PINKY', 'LEFT_INDEX', \n",
    "                'RIGHT_INDEX', 'LEFT_THUMB', 'RIGHT_THUMB', 'LEFT_HIP', 'RIGHT_HIP', \n",
    "                'LEFT_KNEE', 'RIGHT_KNEE', 'LEFT_ANKLE', 'RIGHT_ANKLE', 'LEFT_HEEL', \n",
    "                'RIGHT_HEEL', 'LEFT_FOOT_INDEX', 'RIGHT_FOOT_INDEX']\n",
    "\n",
    "markershands = ['LEFT_WRIST', 'LEFT_THUMB_CMC', 'LEFT_THUMB_MCP', 'LEFT_THUMB_IP', 'LEFT_THUMB_TIP', 'LEFT_INDEX_FINGER_MCP',\n",
    "              'LEFT_INDEX_FINGER_PIP', 'LEFT_INDEX_FINGER_DIP', 'LEFT_INDEX_FINGER_TIP', 'LEFT_MIDDLE_FINGER_MCP', \n",
    "               'LEFT_MIDDLE_FINGER_PIP', 'LEFT_MIDDLE_FINGER_DIP', 'LEFT_MIDDLE_FINGER_TIP', 'LEFT_RING_FINGER_MCP', \n",
    "               'LEFT_RING_FINGER_PIP', 'LEFT_RING_FINGER_DIP', 'LEFT_RING_FINGER_TIP', 'LEFT_PINKY_FINGER_MCP', \n",
    "               'LEFT_PINKY_FINGER_PIP', 'LEFT_PINKY_FINGER_DIP', 'LEFT_PINKY_FINGER_TIP',\n",
    "              'RIGHT_WRIST', 'RIGHT_THUMB_CMC', 'RIGHT_THUMB_MCP', 'RIGHT_THUMB_IP', 'RIGHT_THUMB_TIP', 'RIGHT_INDEX_FINGER_MCP',\n",
    "              'RIGHT_INDEX_FINGER_PIP', 'RIGHT_INDEX_FINGER_DIP', 'RIGHT_INDEX_FINGER_TIP', 'RIGHT_MIDDLE_FINGER_MCP', \n",
    "               'RIGHT_MIDDLE_FINGER_PIP', 'RIGHT_MIDDLE_FINGER_DIP', 'RIGHT_MIDDLE_FINGER_TIP', 'RIGHT_RING_FINGER_MCP', \n",
    "               'RIGHT_RING_FINGER_PIP', 'RIGHT_RING_FINGER_DIP', 'RIGHT_RING_FINGER_TIP', 'RIGHT_PINKY_FINGER_MCP', \n",
    "               'RIGHT_PINKY_FINGER_PIP', 'RIGHT_PINKY_FINGER_DIP', 'RIGHT_PINKY_FINGER_TIP']\n",
    "facemarks = [str(x) for x in range(478)] #there are 478 points for the face mesh (see google holistic face mesh info for landmarks)\n",
    "\n",
    "print(\"Note that we have the following number of pose keypoints for markers body\")\n",
    "print(len(markersbody))\n",
    "\n",
    "print(\"\\n Note that we have the following number of pose keypoints for markers hands\")\n",
    "print(len(markershands))\n",
    "\n",
    "print(\"\\n Note that we have the following number of pose keypoints for markers face\")\n",
    "print(len(facemarks ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make empty objects with column names for face, body, and hands separetely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### set up the column names and objects for the time series data (add time as the first variable/column)=======================\n",
    "markerxyzbody = ['time']\n",
    "markerxyzhands = ['time']\n",
    "markerxyzface = ['time']\n",
    "\n",
    "for mark in markersbody:\n",
    "    for pos in ['X', 'Y', 'Z', 'visibility']: #for markers of the body you also have a visibility reliability score\n",
    "        nm = pos + \"_\" + mark\n",
    "        markerxyzbody.append(nm)\n",
    "for mark in markershands:\n",
    "    for pos in ['X', 'Y', 'Z']:\n",
    "        nm = pos + \"_\" + mark\n",
    "        markerxyzhands.append(nm)\n",
    "for mark in facemarks:\n",
    "    for pos in ['X', 'Y', 'Z']:\n",
    "        nm = pos + \"_\" + mark\n",
    "        markerxyzface.append(nm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions to convert google classification object to numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if there are numbers in a string\n",
    "def num_there(s):\n",
    "    return any(i.isdigit() for i in s)\n",
    "\n",
    "#take some google classification object and convert it into a string\n",
    "def makegoginto_str(gogobj):\n",
    "    gogobj = str(gogobj).strip(\"[]\")\n",
    "    gogobj = gogobj.split(\"\\n\")\n",
    "    return(gogobj[:-1]) #ignore last element as this has nothing\n",
    "\n",
    "#make the stringifyd position traces into clean numerical values\n",
    "def listpostions(newsamplemarks):\n",
    "    newsamplemarks = makegoginto_str(newsamplemarks)\n",
    "    tracking_p = []\n",
    "    for value in newsamplemarks:\n",
    "        if num_there(value):\n",
    "            stripped = value.split(':', 1)[1]\n",
    "            stripped = stripped.strip() #remove spaces in the string if present\n",
    "            tracking_p.append(stripped) #add to this list  \n",
    "    return(tracking_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's run mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mediapipe(vidf, output_video_folder, output_ts_folder):\n",
    "    # get the last part of the path, which is the video name. Note that on Windows you might need to change the slash to a backslash.\n",
    "    videoname = vidf.split(\"\\\\\")[-1] if \"\\\\\" in vidf else vidf.split(\"/\")[-1]\n",
    "    outputf_ts_processed = output_ts_folder + videoname.split(\".mp4\")[0] + \"/\"\n",
    "\n",
    "    #capture the video, and check video settings\n",
    "    capture = cv2.VideoCapture(vidf) #load in the videocapture\n",
    "    frameWidth = capture.get(cv2.CAP_PROP_FRAME_WIDTH) #check frame width\n",
    "    frameHeight = capture.get(cv2.CAP_PROP_FRAME_HEIGHT) #check frame height\n",
    "    samplerate = capture.get(cv2.CAP_PROP_FPS)   #fps = frames per second\n",
    "\n",
    "    #make an 'empty' video file where we project the pose tracking on\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'MP4V') #for different video formats you could use e.g., *'XVID'\n",
    "    out = cv2.VideoWriter(output_video_folder+videoname, fourcc, \n",
    "                            fps = samplerate, frameSize = (int(frameWidth), int(frameHeight)))\n",
    "\n",
    "    # Run MediaPipe frame by frame using Holistic with `enable_segmentation=True` to get pose segmentation.\n",
    "    time = 0\n",
    "    tsbody = [markerxyzbody]   #these will be your time series objects, which start with collumn names initialized above\n",
    "    tshands = [markerxyzhands] #these will be your time series objects, which start with collumn names initialized above\n",
    "    tsface = [markerxyzface]   #these will be your time series objects, which start with collumn names initialized above\n",
    "    with mp_holistic.Holistic(\n",
    "            static_image_mode=False, smooth_landmarks = True, enable_segmentation=True, refine_face_landmarks=True, \n",
    "            min_tracking_confidence = 0.7, model_complexity=1) as holistic:\n",
    "        while (True):\n",
    "            ret, image = capture.read() #read frame\n",
    "            if ret == True: #if there is a frame\n",
    "                # To improve performance, optionally mark the image as not writeable to pass by reference.\n",
    "                image.flags.writeable = False\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) #make sure the image is in RGB format\n",
    "                results = holistic.process(image) #apply Mediapipe holistic processing\n",
    "\n",
    "                # Draw landmark annotation on the image.\n",
    "                image.flags.writeable = True\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "                if  np.all(results.segmentation_mask) != None: #check if there is a pose found\n",
    "                    #now lets draw on the original_image the left and right hand landmarks, the facemesh and the body poses\n",
    "                    #left hand\n",
    "                    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "                    #right hand\n",
    "                    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "                    #face\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        image,\n",
    "                        results.face_landmarks,\n",
    "                        mp_holistic.FACEMESH_TESSELATION,\n",
    "                        landmark_drawing_spec=None,\n",
    "                        connection_drawing_spec=mp_drawing_styles\n",
    "                        .get_default_face_mesh_tesselation_style())\n",
    "                    #body\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        image,\n",
    "                        results.pose_landmarks,\n",
    "                        mp_holistic.POSE_CONNECTIONS,\n",
    "                        landmark_drawing_spec=mp_drawing_styles.\n",
    "                        get_default_pose_landmarks_style())\n",
    "                    \n",
    "                    #######################now save everything to a time series\n",
    "                    #make a variable list with x, y, z, info where data is appended to\n",
    "                    samplebody = listpostions(results.pose_landmarks)\n",
    "                    sampleface = listpostions(results.face_landmarks)\n",
    "                    sampleLH = listpostions(results.left_hand_landmarks)\n",
    "                    sampleRH = listpostions(results.right_hand_landmarks)\n",
    "                    if len(sampleLH) == 0:\n",
    "                        sampleLH = ['' for x in range(int(len(markerxyzhands)/2))]\n",
    "                    samplehands = sampleLH + sampleRH\n",
    "\n",
    "                    samplebody.insert(0, time)\n",
    "                    samplehands.insert(0, time)\n",
    "                    sampleface.insert(0, time)\n",
    "\n",
    "                    tsbody.append(samplebody)   #append to the timeseries object\n",
    "                    tshands.append(samplehands) #append to the timeseries object\n",
    "                    tsface.append(sampleface)   #append to the timeseries object\n",
    "                #show the video as we process (you can comment this out, if you want to run this process in the background)\n",
    "                # cv2.imshow(\"resizedimage\", image)\n",
    "                out.write(image) #save the frame to the new masked video\n",
    "                time = time+(1000/samplerate)#update the time variable  for the next frame\n",
    "            if cv2.waitKey(1) == 27: #allow the use of ESCAPE to break the loop\n",
    "                break\n",
    "            if ret == False: #if there are no more frames, break the loop\n",
    "                break\n",
    "\n",
    "    #once done de-initialize all processes\n",
    "    out.release()\n",
    "    capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1) #this is needed to close the window for mac users\n",
    "\n",
    "    ####################################################### data to be written row-wise in csv file\n",
    "    os.makedirs(outputf_ts_processed) #make a folder for the output\n",
    "    \n",
    "    # opening the csv file in 'w+' mode\n",
    "    filebody = open(outputf_ts_processed + videoname[:-4]+'_body.csv', 'w+', newline ='')\n",
    "    #write it\n",
    "    with filebody:    \n",
    "        write = csv.writer(filebody)\n",
    "        write.writerows(tsbody)\n",
    "\n",
    "    # opening the csv file in 'w+' mode\n",
    "    filehands = open(outputf_ts_processed + videoname[:-4]+'_hands.csv', 'w+', newline ='')\n",
    "    #write it\n",
    "    with filehands:\n",
    "        write = csv.writer(filehands)\n",
    "        write.writerows(tshands)\n",
    "\n",
    "    # opening the csv file in 'w+' mode\n",
    "    fileface = open(outputf_ts_processed + videoname[:-4]+'_face.csv', 'w+', newline ='')\n",
    "    #write it\n",
    "    with fileface:    \n",
    "        write = csv.writer(fileface)\n",
    "        write.writerows(tsface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [32:29:06<00:00, 5847.34s/it]   \n"
     ]
    }
   ],
   "source": [
    "for vidf in tqdm(vfiles[26:]):\n",
    "    run_mediapipe(vidf, outputf_video, outputf_ts)\n",
    "\n",
    "# for vidf in tqdm(vfiles_flipped):\n",
    "#     run_mediapipe(vidf, outputf_video_flipped, outputf_ts_flipped)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mediapipe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
